{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "import requests as rq\n",
    "import bs4 as bs4\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vai ser usado para verificar se as info que eu quero estão no arquivo que eu baixei\n",
    "queries = [ \"machine+learning\", \"data+science\", \"kaggle\"]\n",
    "# url que irei usar para biscar esses vídeos\n",
    "url = \"https://www.youtube.com/results?search_query={query}&sp=CAI%253D&p={page}\"\n",
    "# \"search_query\" vai receber a palavra que eu quero procurar dentro da variavel \"query\"\n",
    "# \"&sp\" que é um código que vai ordenar os resultados do amis recente para o mais antigo (data de upload)\n",
    "# \"p\" você não vai ver esse código no endereço da página do Youtube se for fazer uma consulta direto na plataforma, mas com \n",
    "# esse código é possível paginar a busca no Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# que página vou buscar? eu bisco os vídeos que ordeno pelos amis recentes\n",
    "# o resultado de busca e a página do vídeo\n",
    "\n",
    "# precisa pegar a página que retorna os resultados da busca e pegar a pa´gina de cada vídeo que é retornado.\n",
    "## para fazer isso vamos usar biblioteca chamada \"BeautifulSoup\" e \"requests\"\n",
    "# os datasets usados eu não tenho printis para baixar (como no kaggle). Então vou ter que criar COLETORES para as duas\n",
    "# páginas no youtube.\n",
    "\n",
    "# dica de sempre salvar a página que vc pegou com os coletores e está trabalhando no projeto. Motivos:\n",
    "# - sobrecarregar o servidor se acabar testando muitas cisas várias vezes e acabar sendo bloqueado pelo site do youtube\n",
    "## soluçõ: baixar todas as pag que precisamos e dados e depois pensar sobre a parte de processamento. o \"requests\" é uma \n",
    "## biblioteca que faz requisições no servidor de hospedagem e ele tem tanto requisições GET como POST. O \"BeautifulSoup\" será\n",
    "## usado para fazer o parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=1\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 210476-210481: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6a9f9bfe3549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;31m# \"dados_brutos\" e salvo as páginas com os nomes que eu formato usando as informações de \"query\" e \"page\" para\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m# diferenciá-los. Vai salvar neste arquivo o código fonte da página que estou tentando pegar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# vai transformar a página em dados string (.text) e escrever (write) no arquivo que\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;31m# vai ter o nome já determinado acima.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# para não sobrecarregar o servidor uso o \"time.sleep\" para dar uns 2 segundos de intervalo entre uma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 210476-210481: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Criando o Coletor da Página de Busca do Youtube.\n",
    "\n",
    "for query in queries:\n",
    "    for page in range(1, 21): # para cada query eu quero que ele pegue um determinado número de páginas.\n",
    "        urll = url.format( query=query, page=page ) # formata a url com sa info que eu quero. Basicamente é como se eu estivesse\n",
    "        # fazendo manualemente a busca com a palavra que eu quero e salvando a quantidade de páginas que eu desejo.\n",
    "        print( urll ) # faz um print para verificar como estão sendo geradas essas páginas e se está correto.\n",
    "        response = rq.get( urll ) # quando se usa a tecnica GET no \"requests\" ele vai tentar fazer uma requisição no servidor\n",
    "        # do site usando o endereço que está dentro da variavel \"urll\"\n",
    "        \n",
    "        with open( \"./dados_brutos/{}_{}.html\".format(query, page), 'w+' ) as output: # abro um arquivo na dentro da pasta \n",
    "            # \"dados_brutos\" e salvo as páginas com os nomes que eu formato usando as informações de \"query\" e \"page\" para\n",
    "            # diferenciá-los. Vai salvar neste arquivo o código fonte da página que estou tentando pegar.\n",
    "            output.write( response.text ) # vai transformar a página em dados string (.text) e escrever (write) no arquivo que\n",
    "            # vai ter o nome já determinado acima.\n",
    "        time.sleep(2) # para não sobrecarregar o servidor uso o \"time.sleep\" para dar uns 2 segundos de intervalo entre uma \n",
    "        # requisição e outra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 RAW DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Extrator de Dados da Página de Busca do Youtube.\n",
    "\n",
    "# Quando executamos o código na última sessão e analisamos as páginas resultantes vemos que as infkrmações importantes para \n",
    "# pegarmos destas páginas são a príncipio o título e o link do vídeo.\n",
    "\n",
    "for query in queries:\n",
    "    for page in range(1, 21):\n",
    "        with open( \"./dados_brutos/{}_{}.html\".format(query, page), 'r+' ) as inp:\n",
    "            page_html = inp.read()\n",
    "            \n",
    "            parsed = bs4.BeautifulSoup( page_html )\n",
    "            \n",
    "            tags = parsed.findAll( \"a\" )\n",
    "            \n",
    "            for e in tags:\n",
    "                if e.has_attr( \"aria-describedby\" ):\n",
    "                    link = e[ 'href' ]\n",
    "                    title = e[ 'title' ]\n",
    "                    with open( \"parsed_videos.json\", 'a+' ) as output:\n",
    "                        data = {\"link\": link, \"\" }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
