{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "import bs4 as bs4\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vai ser usado para verificar se as info que eu quero estão no arquivo que eu baixei\n",
    "queries = [ \"machine+learning\", \"data+science\", \"kaggle\"]\n",
    "# url que irei usar para biscar esses vídeos\n",
    "url = \"https://www.youtube.com/results?search_query={query}&sp=CAI%253D&p={page}\"\n",
    "# \"search_query\" vai receber a palavra que eu quero procurar dentro da variavel \"query\"\n",
    "# \"&sp\" que é um código que vai ordenar os resultados do amis recente para o mais antigo (data de upload)\n",
    "# \"p\" você não vai ver esse código no endereço da página do Youtube se for fazer uma consulta direto na plataforma, mas com \n",
    "# esse código é possível paginar a busca no Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# que página vou buscar? eu bisco os vídeos que ordeno pelos amis recentes\n",
    "# o resultado de busca e a página do vídeo\n",
    "\n",
    "# precisa pegar a página que retorna os resultados da busca e pegar a pa´gina de cada vídeo que é retornado.\n",
    "## para fazer isso vamos usar biblioteca chamada \"BeautifulSoup\" e \"requests\"\n",
    "# os datasets usados eu não tenho printis para baixar (como no kaggle). Então vou ter que criar COLETORES para as duas\n",
    "# páginas no youtube.\n",
    "\n",
    "# dica de sempre salvar a página que vc pegou com os coletores e está trabalhando no projeto. Motivos:\n",
    "# - sobrecarregar o servidor se acabar testando muitas cisas várias vezes e acabar sendo bloqueado pelo site do youtube\n",
    "## soluçõ: baixar todas as pag que precisamos e dados e depois pensar sobre a parte de processamento. o \"requests\" é uma \n",
    "## biblioteca que faz requisições no servidor de hospedagem e ele tem tanto requisições GET como POST. O \"BeautifulSoup\" será\n",
    "## usado para fazer o parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    for page in range(1, 21): # para cada query eu quero que ele pegue um determinado número de páginas.\n",
    "        urll = url.format( query=query, page=page ) # formata a url com sa info que eu quero. Basicamente é como se eu estivesse\n",
    "        # fazendo manualemente a busca com a palavra que eu quero e salvando a quantidade de páginas que eu desejo.\n",
    "        print( urll ) # faz um print para verificar como estão sendo geradas essas páginas e se está correto.\n",
    "        response = rq.get( urll ) # quando se usa a tecnica GET no \"requests\" ele vai tentar fazer uma requisição no servidor\n",
    "        # do site usando o endereço que está dentro da variavel \"urll\"\n",
    "        \n",
    "        with open( \"./dados_brutos/{}_{}.html\".format(query, page), 'r+' ) as inp: # abro um arquivo na dentro da pasta \n",
    "            # \"dados_brutos\" e salvo as páginas com os nomes que eu formato usando as informações de \"query\" e \"page\" para\n",
    "            # diferenciá-los. Vai salvar neste arquivo o código fonte da página que estou tentando pegar.\n",
    "            output.write( response.text ) # vai transformar a página em dados string (.text) e escrever (write) no arquivo que\n",
    "            # vai ter o nome já determinado acima.\n",
    "        time.sleep(2) # para não sobrecarregar o servidor uso o \"time.sleep\" para dar uns 2 segundos de intervalo entre uma \n",
    "        # requisição e outra.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 RAW DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    for page in range(1, 21):\n",
    "        with open( \"./dados_brutos/{}_{}.html\".format(query, page), 'r+' ) as inp:\n",
    "            page_html = inp.read()\n",
    "            \n",
    "            parsed = bs4.BeautifulSoup( page_html )\n",
    "            \n",
    "            tags = parsed.findAll( \"a\" )\n",
    "            \n",
    "            for e in tags:\n",
    "                if e.has_attr( \"aria-describedby\" ):\n",
    "                    link = e[ 'href' ]\n",
    "                    title = e[ 'title' ]\n",
    "                    with open( \"parsed_videos.json\", 'a+' ) as output:\n",
    "                        data = {\"link\": link, \"\" }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
