{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "import requests as rq\n",
    "import bs4 as bs4\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vai ser usado para verificar se as info que eu quero estão no arquivo que eu baixei\n",
    "queries = [ \"machine+learning\", \"data+science\", \"kaggle\"]\n",
    "# url que irei usar para biscar esses vídeos\n",
    "url = \"https://www.youtube.com/results?search_query={query}&sp=CAI%253D&p={page}\"\n",
    "# \"search_query\" vai receber a palavra que eu quero procurar dentro da variavel \"query\"\n",
    "# \"&sp\" que é um código que vai ordenar os resultados do amis recente para o mais antigo (data de upload)\n",
    "# \"p\" você não vai ver esse código no endereço da página do Youtube se for fazer uma consulta direto na plataforma, mas com \n",
    "# esse código é possível paginar a busca no Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# que página vou buscar? eu bisco os vídeos que ordeno pelos amis recentes\n",
    "# o resultado de busca e a página do vídeo\n",
    "\n",
    "# precisa pegar a página que retorna os resultados da busca e pegar a pa´gina de cada vídeo que é retornado.\n",
    "## para fazer isso vamos usar biblioteca chamada \"BeautifulSoup\" e \"requests\"\n",
    "# os datasets usados eu não tenho printis para baixar (como no kaggle). Então vou ter que criar COLETORES para as duas\n",
    "# páginas no youtube.\n",
    "\n",
    "# dica de sempre salvar a página que vc pegou com os coletores e está trabalhando no projeto. Motivos:\n",
    "# - sobrecarregar o servidor se acabar testando muitas cisas várias vezes e acabar sendo bloqueado pelo site do youtube\n",
    "## soluçõ: baixar todas as pag que precisamos e dados e depois pensar sobre a parte de processamento. o \"requests\" é uma \n",
    "## biblioteca que faz requisições no servidor de hospedagem e ele tem tanto requisições GET como POST. O \"BeautifulSoup\" será\n",
    "## usado para fazer o parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=1\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=2\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=3\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=4\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=5\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=6\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=7\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=8\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=9\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=10\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=11\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=12\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=13\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=14\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=15\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=16\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=17\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=18\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=19\n",
      "https://www.youtube.com/results?search_query=machine+learning&sp=CAI%253D&p=20\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=1\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=2\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=3\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=4\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=5\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=6\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=7\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=8\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=9\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=10\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=11\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=12\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=13\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=14\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=15\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=16\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=17\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=18\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=19\n",
      "https://www.youtube.com/results?search_query=data+science&sp=CAI%253D&p=20\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=1\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=2\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=3\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=4\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=5\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=6\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=7\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=8\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=9\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=10\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=11\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=12\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=13\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=14\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=15\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=16\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=17\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=18\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=19\n",
      "https://www.youtube.com/results?search_query=kaggle&sp=CAI%253D&p=20\n"
     ]
    }
   ],
   "source": [
    "# Criando o Coletor da Página de Busca do Youtube.\n",
    "\n",
    "for query in queries:\n",
    "    for page in range(1, 21): # para cada query eu quero que ele pegue um determinado número de páginas.\n",
    "        urll = url.format( query=query, page=page ) # formata a url com sa info que eu quero. Basicamente é como se eu estivesse\n",
    "        # fazendo manualemente a busca com a palavra que eu quero e salvando a quantidade de páginas que eu desejo.\n",
    "        print( urll ) # faz um print para verificar como estão sendo geradas essas páginas e se está correto.\n",
    "        response = rq.get( urll ) # quando se usa a tecnica GET no \"requests\" ele vai tentar fazer uma requisição no servidor\n",
    "        # do site usando o endereço que está dentro da variavel \"urll\"\n",
    "        \n",
    "        with open( \"./dados_brutos/{}_{}.html\".format(query, page), 'w+', encoding=\"utf-8\" ) as output: # abro um arquivo \n",
    "            # na dentro da pasta \"dados_brutos\" e salvo as páginas com os nomes que eu formato usando as informações de \"query\" \n",
    "            # e \"page\" para diferenciá-los. Vai salvar neste arquivo o código fonte da página que estou tentando pegar.\n",
    "            output.write( response.text ) # vai transformar a página em dados string (.text) e escrever (write) no arquivo que\n",
    "            # vai ter o nome já determinado acima.\n",
    "        time.sleep(2) # para não sobrecarregar o servidor uso o \"time.sleep\" para dar uns 2 segundos de intervalo entre uma \n",
    "        # requisição e outra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 RAW DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Extrator de Dados da Página de Busca do Youtube.\n",
    "\n",
    "# Quando executamos o código na última sessão e analisamos as páginas resultantes vemos que as infkrmações importantes para \n",
    "# pegarmos destas páginas são a príncipio o título e o link do vídeo.\n",
    "\n",
    "for query in queries:\n",
    "    for page in range(1, 21): # vai interar por todas as páginas.\n",
    "        with open( \"./dados_brutos/{}_{}.html\".format(query, page), 'r+', encoding=\"utf-8\" ) as inp: \n",
    "            page_html = inp.read() # vai ler os arquivos presentes nestas páginas e guardar na variável \"page_html\"\n",
    "            \n",
    "            parsed = bs4.BeautifulSoup( page_html ) # aqui eu uso o BeautifulSoup para entrar na estrutura da página e ajudar\n",
    "            # a encontrar as informações que nós queremos.\n",
    "            \n",
    "            tags = parsed.findAll( \"a\" ) # vai enontrar todas as tagas com o nome presente dentro dela. Neste caso, como\n",
    "            # queremos achar as tags links, usamos o \"a\". No HTML as tags links são representadas por \"a\".\n",
    "            \n",
    "            for e in tags: # vai interar por todos os atributos dentro da variavel \"tags\" onde estão presenets as tags \"a\".\n",
    "                if e.has_attr( \"aria-describedby\" ): # Dentro do browser, vasculhando o código presente nas páginas do Youtube,\n",
    "                    # vc chega a conclusão que o atributo que está sendo procurado aqui e que se chama \"aria-describedby\" está\n",
    "                    # presente nos links retornados e que são os que eu quero obter.\n",
    "                    # Nesse primeiro momento vc verifica se a tag \"a\" tem esse elemento e se sim vc começa a pegar algumas info.\n",
    "                    link = e[ 'href' ] # pega o endereço do link\n",
    "                    title = e[ 'title' ] # pega o título do link\n",
    "                    with open( \"parsed_videos.json\", 'a+', encoding=\"utf-8\" ) as output: # depois eu vou salvar essas \n",
    "                        # infromaçoes em arquivos formato json que são arquivos tipo dicionários e com isso ajuda a dar maior \n",
    "                        # flexibilidade na quantidade de info que veem da busca. ficando mais adaptável a possíveis mudanças.\n",
    "                        data = {\"link\": link, \"title\": title, \"query\": query } # cria o dicionário que será usado no json com \n",
    "                        # as info importantes para se ter os vídeos: link, título e qual a query.\n",
    "                        output.write( \"{}\\n\".format( json.dumps(data) ) ) # vai escrever o arquivo usando o \"dump\" que\n",
    "                        # basicamente transforma o dicionário em string json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 RESULT VERIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.youtube.com/about/\" slot=\"guide-links-primary\" style=\"display: none;\">Sobre</a>,\n",
       " <a href=\"https://www.youtube.com/about/press/\" slot=\"guide-links-primary\" style=\"display: none;\">Imprensa</a>,\n",
       " <a href=\"https://www.youtube.com/about/copyright/\" slot=\"guide-links-primary\" style=\"display: none;\">Direitos autorais</a>,\n",
       " <a href=\"/t/contact_us\" slot=\"guide-links-primary\" style=\"display: none;\">Entre em contato</a>,\n",
       " <a href=\"https://www.youtube.com/creators/\" slot=\"guide-links-primary\" style=\"display: none;\">Criadores de conteúdo</a>,\n",
       " <a href=\"https://www.youtube.com/ads/\" slot=\"guide-links-primary\" style=\"display: none;\">Publicidade</a>,\n",
       " <a href=\"https://developers.google.com/youtube\" slot=\"guide-links-primary\" style=\"display: none;\">Desenvolvedores</a>,\n",
       " <a href=\"/t/terms\" slot=\"guide-links-secondary\" style=\"display: none;\">Termos</a>,\n",
       " <a href=\"https://www.google.com.br/intl/pt-BR/policies/privacy/\" slot=\"guide-links-secondary\" style=\"display: none;\">Privacidade</a>,\n",
       " <a href=\"https://www.youtube.com/about/policies/\" slot=\"guide-links-secondary\" style=\"display: none;\">Política e Segurança</a>,\n",
       " <a href=\"/new\" slot=\"guide-links-secondary\" style=\"display: none;\">Testar os novos recursos</a>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.findAll( \"a\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 COLECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ed86e3864d3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"parsed_video.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Create_DataScience_Soluction\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Create_DataScience_Soluction\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Create_DataScience_Soluction\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Create_DataScience_Soluction\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Create_DataScience_Soluction\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Create_DataScience_Soluction\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1089\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m             )\n\u001b[0;32m   1091\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "df = pd.read_json( \"parsed_video.json\", lines=True )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Extrator de Dados da Página de Busca do Youtube."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
